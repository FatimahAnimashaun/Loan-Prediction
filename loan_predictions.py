# -*- coding: utf-8 -*-
"""Loan Predictions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EQVuSjX6UZ5ccO20ZMX0trIpoUAJZRFw
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load train data

train_data = pd.read_csv('train.csv')

train_data.head()

train_data.info()

train_data.shape

#Check for missing values

train_data.isnull().sum()

unique_values = {}
for column in train_data.columns:
    unique_values[column] = train_data[column].value_counts().shape[0]

pd.DataFrame(unique_values, index=['unique value count']).transpose()

for column in train_data:
    print(train_data[column].value_counts())
    print("_" * 50)

"""The train data contains 614 records with the following columns: Gender, Married, Dependents, Education, Self_Employed, ApplicationIncome, etc.

The dataset also has missing values in columns like Gender, Married, Dependents, Self-Employed, LoanAmount, etc
"""

#load the test data

test_data = pd.read_csv('test.csv')

test_data.head()

test_data.info()

test_data.shape

test_data.isnull().sum()

"""The test data contains 367 records with the following columns: Gender, Married, Dependents, Education, Self_Employed, ApplicationIncome, etc.

The dataset also has missing values in columns like Gender, Married, Dependents, Self-Employed, LoanAmount, etc
"""

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder

# Fill missing numerical columns with the median
num_imputer = SimpleImputer(strategy='median')
train_data['LoanAmount'] = num_imputer.fit_transform(train_data[['LoanAmount']])
test_data['LoanAmount'] = num_imputer.transform(test_data[['LoanAmount']])

train_data['Loan_Amount_Term'] = num_imputer.fit_transform(train_data[['Loan_Amount_Term']])
test_data['Loan_Amount_Term'] = num_imputer.transform(test_data[['Loan_Amount_Term']])

train_data['Credit_History'] = num_imputer.fit_transform(train_data[['Credit_History']])
test_data['Credit_History'] = num_imputer.transform(test_data[['Credit_History']])

# Fill missing categorical columns with the most frequent value
cat_imputer = SimpleImputer(strategy='most_frequent')
train_data['Gender'] = cat_imputer.fit_transform(train_data[['Gender']]).ravel()
test_data['Gender'] = cat_imputer.transform(test_data[['Gender']]).ravel()

train_data['Married'] = cat_imputer.fit_transform(train_data[['Married']]).ravel()
test_data['Married'] = cat_imputer.transform(test_data[['Married']]).ravel()

train_data['Dependents'] = cat_imputer.fit_transform(train_data[['Dependents']]).ravel()
test_data['Dependents'] = cat_imputer.transform(test_data[['Dependents']]).ravel()

train_data['Self_Employed'] = cat_imputer.fit_transform(train_data[['Self_Employed']]).ravel()
test_data['Self_Employed'] = cat_imputer.transform(test_data[['Self_Employed']]).ravel()

# Encoding categorical variables
label_encoder = LabelEncoder()

categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area']

# Apply label encoding for each categorical column
for column in categorical_columns:
    train_data[column] = label_encoder.fit_transform(train_data[column])
    test_data[column] = label_encoder.transform(test_data[column])

# Encoding the target variable
train_data['Loan_Status'] = label_encoder.fit_transform(train_data['Loan_Status'])

# Check if preprocessing was successful
train_data.head(), test_data.head()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Defining the features and target variable
X = train_data.drop(columns=['Loan_ID', 'Loan_Status'])
y = train_data['Loan_Status']

# Splitting the data into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Creating and training the Logistic Regression model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Making predictions on the validation set
y_pred = model.predict(X_val)

# Evaluating the model
accuracy = accuracy_score(y_val, y_pred)
classification_rep = classification_report(y_val, y_pred)

# Displaying the accuracy and classification report
print(f"Validation Accuracy: {accuracy}")
print(f"Classification Report:\n{classification_rep}")

"""Model Evaluation Results: Validation Accuracy: 78%"""

# Prepare the test dataset for prediction (drop Loan_ID as it's not used for prediction)
X_test = test_data.drop(columns=['Loan_ID'])

# Make predictions on the test data
test_data['Loan_Status_Prediction'] = model.predict(X_test)

# Convert the numeric predictions back to the original labels
test_data['Loan_Status_Prediction'] = label_encoder.inverse_transform(test_data['Loan_Status_Prediction'])

# Save the predictions to a CSV file
output = test_data[['Loan_ID', 'Loan_Status_Prediction']]
output.to_csv('loan_predictions.csv', index=False)

# Display the first few rows of the output
output.head()

# Creating and training the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Making predictions on the validation set
y_val_pred = rf_model.predict(X_val)

# Evaluating the model
rf_accuracy = accuracy_score(y_val, y_val_pred)
rf_classification_report = classification_report(y_val, y_val_pred)

print(f"Validation Accuracy: {rf_accuracy}")
print(f"Classification Report:\n{rf_classification_report}")

"""Validation Accuracy: 75%"""

# Ensure 'Loan_Status_Prediction' is not present in X_test
X_test = test_data.drop(columns=['Loan_ID', 'Loan_Status_Prediction'], errors='ignore')

# Make sure categorical columns in test data are processed as they were in training data
for column in categorical_columns:
    if X_test[column].dtype == 'object':
        # Apply the label encoding transformation
        X_test[column] = label_encoder.transform(X_test[column])

# Now, let's try making predictions on the test data again
test_data['Loan_Status_Prediction'] = rf_model.predict(X_test)

# Convert predictions back to original labels
test_data['Loan_Status_Prediction'] = label_encoder.inverse_transform(test_data['Loan_Status_Prediction'])

# Save the predictions to a CSV file
test_output = test_data[['Loan_ID', 'Loan_Status_Prediction']]
test_output.to_csv('loan_predictions_rf_fixed.csv', index=False)

# Display the first few rows of the output
print(test_output.head())